{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1deba879-fe03-486a-a1ba-59bb217651ee",
   "metadata": {},
   "source": [
    "# **Step 10: Study Advanced Units** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3d8cf-0cf5-4901-b741-20d044c0bee3",
   "metadata": {},
   "source": [
    "Some areas where NLP unit reinforced my understanding and progress on my capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5191d622-eeaf-4f1d-a3ac-1f9f110a0899",
   "metadata": {},
   "source": [
    "#### **Text-PreProcessing** ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f39dcb-cd21-415a-8dcc-8809033311b7",
   "metadata": {},
   "source": [
    "I learned to identify more items to clean and how to clean it.\n",
    "\n",
    "- HTML tags\n",
    "- Accented characters\n",
    "- Expanding contractions\n",
    "- Difference between stemming and lemmatization\n",
    "- Text normalizer ( As well as we normalize numbers, we can also normalize words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3b205-e81d-40bb-b711-bbed3bce3420",
   "metadata": {},
   "source": [
    "#### **Feature Engineering: Text Embedding** ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229e0e7d-7898-42c9-b211-4ada017c13c2",
   "metadata": {},
   "source": [
    "I was using only Bag of Words on my earlier models.  I undertand better now what word embedding is and how this reduce computing.  Instead of feeding the entire vocabulary to the model we feed the maximum number of words that are in a sentence.  e.g instead of feeding 1000 (unique words count) entries we feed 7 where 7 is the maximum or words across all sentences on my tweets.  Each entry on the 7 node is represented by a vector that uniquely identify a word within the vocabulary.\n",
    "\n",
    "- I've used the embedding layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92623b0b-fb7a-4ce6-bd07-bc59025034c2",
   "metadata": {},
   "source": [
    "#### **NER** ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7071f10-2949-4321-8a63-a3cf3dd11aac",
   "metadata": {},
   "source": [
    "I learned there's models that can identify words within a sentence.  The location column on my data has a high percentage of missing values.  I can use NER to fill the location column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a162d0-7c4f-4aa1-9644-13ee6998d736",
   "metadata": {},
   "source": [
    "#### **Modeling** ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa0ce9-fd15-4c48-8ddd-af68814eddcc",
   "metadata": {},
   "source": [
    "I've learned that deep learning does not work for small data sizes.\n",
    "The results on the four different models I test were higher on the first three regular machine learning models than on LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac614f66-048f-4886-8e6c-6759f68bea32",
   "metadata": {},
   "source": [
    "Naive Bayes:                  Accuracy = 0.7989452867501649\n",
    "SVM :                         Accuracy = 0.7705998681608438\n",
    "Gradient Boosting Classifier: Accuracy = 0.7844429795649308\n",
    "LSTM:                         Accuracy = 0.7444005012512207"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c26bb3-86cd-4fc4-a460-0f1a79ef761f",
   "metadata": {},
   "source": [
    "#### **Transfer Learning** ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bcbe07-d6a8-48fb-aff7-79968b01f625",
   "metadata": {},
   "source": [
    "Thought the size of my data is small to use Deep Learning models  (<10K rows), I can use pretrain models to extend the vocabulary (word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68619b97-8129-485a-91b5-41056b2534c3",
   "metadata": {},
   "source": [
    "#### **Next steps** ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbea669-8dc4-4d52-9f3e-fd46d0e7506b",
   "metadata": {},
   "source": [
    "Cleaning\n",
    "-----------------------------\n",
    "-accented characters\n",
    "-expanding contractions\n",
    "-lemmatization\n",
    "-removing extra whitespaces\n",
    "\n",
    "Word Embedding\n",
    "-----------------------------\n",
    "-word embedding word2vec\n",
    "-CBOW... check words that take surrounding words as features.   Fire can mean an emergency depending on what are the previous or following words\n",
    "-bert embedding\n",
    "\n",
    "NER for location\n",
    "-------------------------------\n",
    "-NER (Named Entity recognition)\n",
    "-Put back emojis and exclamation and upper case\n",
    "-Transfer learning\n",
    "\n",
    "Modeling\n",
    "-------------------------------\n",
    "-CNN\n",
    "-Sentiment analysis to differentiate between emergencies an non-emergencies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
