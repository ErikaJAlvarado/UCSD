{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e8c2fd-945d-427a-ab58-1eaedfdf8b86",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aba2726-6c14-401c-91cc-fade96a5456f",
   "metadata": {},
   "source": [
    "# **Step 6: Survey Existing Research and Reproduce Available Solutions** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b6661-e024-4086-8c63-f8298d0d6e14",
   "metadata": {},
   "source": [
    "# Reproducing a paper\n",
    "\n",
    "## Paper: Using Social Media to Enhance Emergency Situation Awareness: Extended Abstract\n",
    "## Jie Yin† , Sarvnaz Karimi† , Andrew Lampert‡ , Mark Cameron† ,Bella Robinson† , Robert Power†\n",
    "https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/view/11210/11162"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d99362e-016b-499d-9c01-dc6e64943c63",
   "metadata": {},
   "source": [
    "The paper study three different tweet classification settings: \n",
    "- disaster or not, \n",
    "- disaster type\n",
    "- Impact assesment.\n",
    "\n",
    "We're going to replicate \"disaster or not\"\n",
    "\n",
    "\n",
    "Disaster or not and disaster type clas-\n",
    "sifiers utilized word unigram, word bigram, hashtag, hash-\n",
    "tag count (number of hashtags in a tweet), mention, mention\n",
    "count (number of user mentions in a tweet), link (a binary fea-\n",
    "ture whether or not a link exists in a tweet), and tweet length.\n",
    "\n",
    "For disaster or not and disaster type classifiers, we used a\n",
    "time-split evaluation scheme [Karimi et al., 2015] that pre-\n",
    "vents any biases in evaluations that may occur due to the de-\n",
    "pendencies among Twitter data. In our time-split evaluation,\n",
    "a dataset of 5,747 tweets was sorted in chronological order\n",
    "and divided into two sets of training and testing. Therefore,\n",
    "the training data represents older tweets based on their tweet\n",
    "time. For both of the classifiers, using a combination of hash-\n",
    "tags and word unigrams performed better than other feature\n",
    "combinations once at least 50% of the data was used for train-\n",
    "ing. Once trained on 90% of the data, both of the classifiers\n",
    "were accurate over 90% of the time. SVM classifier consis-\n",
    "tently outperformed Naı̈ve Bayes in all the feature settings\n",
    "and therefore Naı̈ve Bayes was left unreported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c89de96-e195-4206-aa6b-828f2545ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d11864d-e97e-44a1-8aed-ccace70d7d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "directory = \"/home/erika/UCSD/UCSD/capstone/NLP_with_Disaster_Tweets/data/tweeter_disaster_text_analysis\"\n",
    "data_pickle = os.path.join(directory,\"training_data.pkl\")\n",
    "\n",
    "data_df = pd.read_pickle(data_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ef6ba40-6214-45f8-8bb6-b02a043c2803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>ablaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>ablaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>ablaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>\"Philadelphia, PA\"</td>\n",
       "      <td>\"13,000 people receive #wildfires evacuation o...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "      <td>ablaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>\"London, UK\"</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>ablaz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id keyword                       location  \\\n",
       "0  1  ablaze                     Birmingham   \n",
       "1  4  ablaze  Est. September 2012 - Bristol   \n",
       "2  5  ablaze                         AFRICA   \n",
       "3  6  ablaze             \"Philadelphia, PA\"   \n",
       "4  7  ablaze                   \"London, UK\"   \n",
       "\n",
       "                                                text  target  \\\n",
       "0  our deeds are the reason of this #earthquake m...       1   \n",
       "1             forest fire near la ronge sask. canada       1   \n",
       "2  all residents asked to 'shelter in place' are ...       1   \n",
       "3  \"13,000 people receive #wildfires evacuation o...       1   \n",
       "4  just got sent this photo from ruby #alaska as ...       1   \n",
       "\n",
       "                                          clean_text clean_keyword  \n",
       "0          deed reason earthquak may allah forgiv us         ablaz  \n",
       "1               forest fire near la rong sask canada         ablaz  \n",
       "2  resid ask shelter place notifi offic evacu she...         ablaz  \n",
       "3        peopl receiv wildfir evacu order california         ablaz  \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...         ablaz  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5c02c-51fe-4bda-bbd3-e30a1b3261a4",
   "metadata": {},
   "source": [
    "# **CountVectorizer** -   Document-Term matrix #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3ba75e-54f7-4bc8-8ef6-b41e2fe13836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data_df[\"feature\"] = data_df[\"clean_keyword\"] + \" \" + data_df[\"clean_text\"]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit(data_df[\"feature\"])\n",
    "X = cv.transform(data_df[\"feature\"])\n",
    "y = data_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40fd0d7f-30bf-4789-8951-0155aa820f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data.vocabulary_)\n",
    "#print(cv.get_feature_names())\n",
    "#print(data.shape)\n",
    "#print(data.toarray())\n",
    "\n",
    "#Print the position where it's non-zero\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f017fb0-ef5a-4045-8d9f-beec8a225e6f",
   "metadata": {},
   "source": [
    "# **Training/Test data split** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b6d1ba6-326f-44e2-b31f-538a387098c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = .2, random_state = 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd312a6d-f9aa-49c6-8a1d-523f455d3820",
   "metadata": {},
   "source": [
    "# **Naive Bayes** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1af7e2-db5e-44a4-a595-54f1d0116cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "0.7989452867501649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Multi_NB = MultinomialNB()\n",
    "Multi_NB.fit(X_train, y_train)\n",
    "print(Multi_NB)\n",
    "\n",
    "y_pred = Multi_NB.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bed0e-8c04-4e57-bb5f-ce9b38479b26",
   "metadata": {},
   "source": [
    "# **Naive Bayes - Crossvalidation** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a7ff2a-ae03-44c4-924e-88fd872f6ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78995058, 0.79884584, 0.80626546, 0.78318219, 0.79967024,\n",
       "       0.78747941, 0.80049464, 0.78812861, 0.80626546, 0.79225062,\n",
       "       0.78336079, 0.79307502, 0.80626546, 0.80296785, 0.7873042 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold,cross_val_score\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(Multi_NB, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68995f-508c-4561-bcc3-4565716cc8c1",
   "metadata": {},
   "source": [
    "# **SVM** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97825b9d-a168-438d-9957-d55664dbd2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7705998681608438\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm = svm.SVC(kernel=\"linear\")\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
